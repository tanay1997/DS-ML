{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from scipy.sparse import find\n",
    "\n",
    "# Feel free to import any standard libraries that you may need to complete the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step1: ** Fetch the dataset for the three aforementioned categories using scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['talk.religion.misc','comp.graphics','sci.space']\n",
    "\n",
    "num_categories = len(categories)\n",
    "\n",
    "#Loading training data\n",
    "\n",
    "data_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "#Loading testing data\n",
    "\n",
    "data_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "\n",
    "# Loading the class labels for training and testing data\n",
    "\n",
    "y_train, y_test = data_train.target, data_test.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contatins \n",
      " 1554 train documents, \n",
      " 1034 test documents.\n"
     ]
    }
   ],
   "source": [
    "# Total number of documents in train and test datasets\n",
    "\n",
    "num_train = len(data_train.target)\n",
    "num_test = len(data_test.target)\n",
    "\n",
    "print(\"Dataset contatins \\n \"\n",
    "       +str(num_train)+\" train documents, \\n \"\n",
    "       + str(num_test) + \" test documents.\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's print a sample document to understand the dataset better.\n",
    "\n",
    "Fill in the cell below to print contents of the first document from \"train\" subset. Also, print its corresponding class label name(category).\n",
    "\n",
    "**Hint:** Use \"data_train.data\" variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: nicho@vnet.IBM.COM (Greg Stewart-Nicholls)\\nSubject: Re: Biosphere II\\nReply-To: nicho@vnet.ibm.com\\nDisclaimer: This posting represents the poster's views, not those of IBM\\nNews-Software: UReply 3.1\\nX-X-From: nicho@vnet.ibm.com\\n            <1q1kia$gg8@access.digex.net>\\nLines: 18\\n\\nIn <1q1kia$gg8@access.digex.net> Pat writes:\\n>In article <19930408.043740.516@almaden.ibm.com> nicho@vnet.ibm.com writes:\\n>>In <1q09ud$ji0@access.digex.net> Pat writes:\\n>>>Why is everyone being so critical of B2?\\n>> Because it's bogus science, promoted as 'real' science.\\n>It seems to me, that it's sorta a large engineering project more\\n>then a science project.\\n  Bingo.\\n>B2 is not bench science,  but rather a large scale attempt to\\n>re-create a series of micro-ecologies.   what's so eveil about this?\\n Nothing evil at all. There's no actual harm in what they're doing, only\\nhow they represent it.\\n\\n -----------------------------------------------------------------\\n .sig files are like strings ... every yo-yo's got one.\\n\\nGreg Nicholls ... nicho@vnet.ibm.com (business) or\\n                  nicho@olympus.demon.co.uk (private)\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step2:** Remove stop words and create count vectors for the train and test datasets.\n",
    "\n",
    "   We use the CountVectorizer method to extract features (counts for each word). Note that words from both training and testing data are needed to build the count table.\n",
    "   \n",
    "   *Documentation:*  http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vectorizer.fit(data_train.data + data_test.data)\n",
    "x_train = vectorizer.transform(data_train.data)\n",
    "x_test = vectorizer.transform(data_test.data)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = x_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = np.where(y_train == 0)\n",
    "class1 = np.where(y_train == 1)\n",
    "class2 = np.where(y_train == 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = x_train[class0]\n",
    "x1 = x_train[class1]\n",
    "x2 = x_train[class2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "N0 = np.where(x0.toarray()>0)\n",
    "N1 = np.where(x1.toarray()>0)\n",
    "N2 = np.where(x2.toarray()>0)\n",
    "\n",
    "n0 = x0[N0]\n",
    "n1 = x1[N1]\n",
    "n2 = x2[N2]\n",
    "\n",
    "Z0 = np.asarray(n0).sum(axis=1)\n",
    "Z1 = np.asarray(n1).sum(axis=1)\n",
    "Z2 = np.asarray(n2).sum(axis=1)\n",
    "\n",
    "\n",
    "NC = [Z0, Z1, Z2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.zeros((num_categories,x_train.shape[1]))\n",
    "\n",
    "\n",
    "for i in range(x0.shape[1]):\n",
    "    c[0,i] = np.asarray(x0[:,i].toarray()).sum(axis=0)        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x0.shape[1]):\n",
    "    c[1,i] = np.asarray(x1[:,i].toarray()).sum(axis=0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x0.shape[1]):\n",
    "    c[2,i] = np.asarray(x2[:,i].toarray()).sum(axis=0)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build a Multinomial Naive Bayes classifier that takes feature vector from the test data as input and classifies as one of the three classes ('talk.religion.misc','comp.graphics','sci.space').\n",
    "\n",
    "Complete the training function MultiNB_train() in the cell below to train a Multiomial Naive Bayes classifier that takes \"x_train\",\"y_train\",\"alpha\" as inputs and returns the likelihood probability matrix \"theta\" and the prior distribution  \"prior\" on the document category.\n",
    "\n",
    "\"prior\" is a vector of length equal to num_categories where the $i$-th element is defined as\n",
    "$$ prior (i) = \\frac{\\text{ # of train documents with category i}}{\\text{Total number of train documets}} $$\n",
    "\n",
    "\"theta\" ($\\theta$) is the  matrix with the $(c,i)$th element defined by\n",
    "\n",
    " $$ \\theta(c,i) = P(w_i/c) =  \\frac{N_{ci} + \\alpha }{N_c + |V| \\alpha}$$\n",
    " \n",
    " where,\n",
    " * $P(w_i/c)$ refers to the probability of seeing the $i$th word in the vocabulary given that class type is $c$.\n",
    " * $N_{ci}$ refers to the total number of times the word  $i$ appeared in the training documents of class type $c$.\n",
    " * $N_c$ is the total number of words in the documents of type $c$\n",
    "    $$N_c = \\sum_{d \\in T[c]} N_{cd}$$\n",
    "    where, $T[c]$ refers to the documents of type $c$.\n",
    " * $|V|$ is the size of the vocabulary.\n",
    " * $\\alpha$ is the laplace smoothing parameter\n",
    "\n",
    "***Note**: **Do NOT** use the scikit-learn's inbuilt function \"MultinomialNB\" . Write your own code to build the classifier. You may use standary libraries like \"numpy\",\"scipy\" etc. to perform operations on matrices/arrays. \n",
    "\n",
    "Feel free to break your code into multiple functions or cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiNB_train(x_train,y_train, alpha):\n",
    "    prior = np.bincount(y_train)/num_train\n",
    "    theta = np.zeros((num_categories,x_train.shape[1]))\n",
    "    for j in range (num_categories):\n",
    "        for i in range(x_train.shape[1]):\n",
    "            theta[j,i] = (c[j,i] + alpha)/(NC[j] + (V*alpha)) \n",
    "        \n",
    "    return(theta, prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us train the model to learn the likelihood parameters $\\theta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta, prior = MultiNB_train(x_train,y_train,alpha = 0.58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the classifier function MultiNB_classify() below that takes in features of one test sample (one row from x_test) and returns the predicted class \"pred_class\" $\\in \\{0,1,2\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiNB_classify(x_test_sample, theta, prior):\n",
    "    \n",
    "    \n",
    "        id = np.where(x_test_sample.toarray() > 0)\n",
    "        p = x_test_sample[id]\n",
    "       \n",
    "        r0 = np.log(theta[0,id[1]])\n",
    "        r1 = np.log(theta[1,id[1]])\n",
    "        r2 = np.log(theta[2,id[1]])\n",
    "        r = np.matrix([r0, r1, r2])\n",
    "        g = np.matmul(r, p.transpose())\n",
    "        P = g.sum(axis=1)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        P0 = np.log(prior[0])+P[0]\n",
    "        P1 = np.log(prior[1])+P[1]\n",
    "        P2 = np.log(prior[2])+P[2]\n",
    "        u = np.array([P0, P1, P2])\n",
    "        pred_class = np.argmax(u)\n",
    "\n",
    "    \n",
    "\n",
    "        return pred_class\n",
    "    \n",
    "   \n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test our classifier on the first sample of testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class:0\n",
      "actual class:0\n"
     ]
    }
   ],
   "source": [
    "pred_class = MultiNB_classify(x_test.getrow(0),theta, prior)\n",
    "\n",
    "print(\"predicted class:\" + str(pred_class))\n",
    "print(\"actual class:\" + str(y_test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluating the classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code below runs your classifier on every data sample from the testing dataset and stored them in \"y_pred\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "for i in range(num_test):\n",
    "    pred_class = MultiNB_classify(x_test.getrow(i),theta=theta, prior= prior)\n",
    "    y_pred.append(pred_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell evaluates your result by comparing it with the test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.958\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.95      0.96      0.96       389\n",
      "          1       0.96      0.95      0.96       394\n",
      "          2       0.97      0.96      0.96       251\n",
      "\n",
      "avg / total       0.96      0.96      0.96      1034\n",
      "\n"
     ]
    }
   ],
   "source": [
    "score = metrics.accuracy_score(y_test,y_pred)\n",
    "\n",
    "print(\"accuracy: %0.3f\" % score)\n",
    "print(metrics.classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the classification error (1-score) over the test set for various values of the smoothing parameter α and by trial and error find a good value of α."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A good alpha value is between 0.4 to 0.7\n"
     ]
    }
   ],
   "source": [
    "print('A good alpha value is between 0.4 to 0.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
